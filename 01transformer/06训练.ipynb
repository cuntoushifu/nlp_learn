{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-24T04:40:34.816552Z",
     "start_time": "2024-03-24T04:40:34.772988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# cache_dir = \"/Users/yangye/models\"\n",
    "# 设置环境变量\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "# 检查环境变量是否设置成功\n",
    "print(os.environ[\"HF_ENDPOINT\"])\n",
    "# 设置 CUDA_VISIBLE_DEVICES 环境变量\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/yangye/miniconda3/envs/nlp_learn/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# This is new\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T04:40:38.300551Z",
     "start_time": "2024-03-24T04:40:34.831166Z"
    }
   },
   "id": "292e08987e6dc6cc",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1725\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T04:40:59.875027Z",
     "start_time": "2024-03-24T04:40:38.301562Z"
    }
   },
   "id": "ad445f7ffc79567",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n 'label': 1,\n 'idx': 0}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T04:40:59.883804Z",
     "start_time": "2024-03-24T04:40:59.877062Z"
    }
   },
   "id": "a0bc16cc449f5848",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'sentence1': Value(dtype='string', id=None),\n 'sentence2': Value(dtype='string', id=None),\n 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n 'idx': Value(dtype='int32', id=None)}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T04:40:59.890959Z",
     "start_time": "2024-03-24T04:40:59.886571Z"
    }
   },
   "id": "ebe8f98152d356a2",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'], 'validation': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'], 'test': ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask']}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return  tokenizer(\n",
    "        example[\"sentence1\"],example[\"sentence2\"], padding=\"max_length\", truncation=True,max_length=128\n",
    "    )\n",
    "\n",
    "tokenizer_datasets=raw_datasets.map(tokenize_function,batched=True)\n",
    "print(tokenizer_datasets.column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T04:41:00.308746Z",
     "start_time": "2024-03-24T04:40:59.892221Z"
    }
   },
   "id": "6bf26c51841507f3",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['sentence1', 'sentence2', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 3668\n})"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_datasets=tokenizer_datasets.remove_columns([\"idx\"])\n",
    "tokenizer_datasets=tokenizer_datasets.rename_column(\"label\",\"labels\")\n",
    "tokenizer_datasets=tokenizer_datasets.with_format(\"torch\")\n",
    "\n",
    "tokenizer_datasets[\"train\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T04:41:00.323600Z",
     "start_time": "2024-03-24T04:41:00.309660Z"
    }
   },
   "id": "7790fd2b900ce6c7",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tokenizer_datasets[\"train\"].select(range(1000))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T11:35:07.298059Z",
     "start_time": "2024-03-24T11:35:07.242566Z"
    }
   },
   "id": "e9c549f5a7ea118a",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(tokenizer_datasets[\"train\"], shuffle=True, batch_size=16)\n",
    "\n",
    "for step,batch in enumerate(train_dataloader):\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    if step>5:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T11:37:17.648841Z",
     "start_time": "2024-03-24T11:37:17.613172Z"
    }
   },
   "id": "1f08489b1e02e5b6",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/408 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3901a32eb56c45a881b82c6fc4cde73c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 3668\n})"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from  transformers import AutoTokenizer\n",
    "\n",
    "raw_datasets=load_dataset(\"glue\",\"mrpc\")\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"],example[\"sentence2\"],truncation=True)\n",
    "\n",
    "tokenizer_datasets=raw_datasets.map(tokenize_function,batched=True)\n",
    "tokenizer_datasets=tokenizer_datasets.remove_columns([\"idx\",\"sentence1\",\"sentence2\"])\n",
    "tokenizer_datasets=tokenizer_datasets.rename_column(\"label\",\"labels\")\n",
    "tokenizer_datasets=tokenizer_datasets.with_format(\"torch\")\n",
    "\n",
    "tokenizer_datasets[\"train\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T11:44:44.472810Z",
     "start_time": "2024-03-24T11:44:23.135583Z"
    }
   },
   "id": "3f499f0529ae0907",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 71])\n",
      "torch.Size([8, 69])\n",
      "torch.Size([8, 72])\n",
      "torch.Size([8, 73])\n",
      "torch.Size([8, 72])\n",
      "torch.Size([8, 71])\n",
      "torch.Size([8, 84])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator=DataCollatorWithPadding(tokenizer)\n",
    "train_dataloader=DataLoader(tokenizer_datasets[\"train\"],shuffle=True,batch_size=8,collate_fn=data_collator)\n",
    "\n",
    "for step,batch in enumerate(train_dataloader):\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    if step>5:\n",
    "        break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T12:08:09.704657Z",
     "start_time": "2024-03-24T12:08:09.592411Z"
    }
   },
   "id": "1287ec06f54a96bb",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7bdea3a22508cfe7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
